# -*- coding: utf-8 -*-
"""satellite_to_roadmap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdjRE38gcEHM_mL_C7m5ltTEUx7gOmkF
"""

!pip install torchinfo

import torch
import torchinfo
import torchvision
import os
from PIL import Image
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import torch.nn as nn
from torch.utils.data import default_collate
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.pyplot as plt
##%%
height = 256
width = 256
device =  torch.device("cuda" if torch.cuda.is_available() else "cpu")
lr = 2e-4
batch_size = 32
Lambda = 100
epoch = 300

generator = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',
    in_channels=3, out_channels=3, init_features=64, pretrained=False)
gen = generator.to(device)

class Discriminator(torch.nn.Module):
    def __init__(self, in_channels, features = 32):
        super( Discriminator, self).__init__()
        self.disc = nn.Sequential(
                  nn.Conv2d(
                        in_channels*2,
                        features,
                        kernel_size=4,
                        stride=2,
                        padding=1,
                        padding_mode='reflect'
                  ),
                  nn.LeakyReLU(0.2),
                  self._block(features,features*2,4,2,1),
                  self._block(features*2,features*4,4,2,1),
                  self._block(features*4,features*8,4,2,1),
                  nn.Conv2d(
                        features*8,
                        1,
                        kernel_size=4,
                        stride=2,
                        padding=0,
                        padding_mode='reflect'
                  ),
                  nn.Sigmoid(),

            )
    def _block(self, in_channels , features, kernel_size , stride , padding):
        return nn.Sequential(
                  nn.Conv2d(in_channels,
                            features,
                            kernel_size,
                            stride,
                            padding,
                            bias=False,
                            padding_mode='reflect'
                            ),
                  nn.BatchNorm2d(features),
                  nn.LeakyReLU(0.2)
            )
    def forward(self,x, y):
        x = torch.cat([x, y] , axis = 1 )
        return self.disc(x)

class dataset(Dataset):
    def __init__(self ,root, width, height, transforms):
        self.files = os.listdir(root)
        self.root = root
        self.width = width
        self.height = height
        self.transforms = transforms
    def __len__(self):
        return len(self.files)

    def __getitem__(self, index):
        img = Image.open(f'{self.root}/{self.files[index]}')
        img = img.resize((self.width*2, self.height))
        x = img.crop((0,0, self.width , self.height))
        y = img.crop((self.width, 0 , self.width*2, self.height))
        if self.transforms is not None:
            x = self.transforms(x)
            y = self.transforms(y)
        return x,y

transforms = torchvision.transforms.Compose([
                                             torchvision.transforms.ToTensor(),

                                            ])

Train_set= dataset("/content/drive/MyDrive/archive (5)/train", width , height, transforms)
Valid_set = dataset("/content/drive/MyDrive/archive (5)/val", width , height, transforms)

Train_data = DataLoader(Train_set, batch_size = batch_size, shuffle = True)

torchinfo.summary(gen)

torchinfo.summary(Discriminator(3,128))

disc = Discriminator(3).to(device)
gen_opt = torch.optim.Adam(gen.parameters() , lr = lr, betas = (0.5 , 0.999))
disc_opt = torch.optim.Adam(disc. parameters(), lr = lr , betas = (0.5 , 0.999))
Log_loss = nn.BCELoss()
L1_loss = nn.L1Loss()

for i in range(epoch):
    torch.save({
            'gen_model_state_dict': gen.state_dict(),
            'disc_model_state_dict': disc.state_dict(),
            'gen_optimizer_state_dict': gen_opt.state_dict(),
            'disc_optimizer_state_dict': disc_opt.state_dict(),
            }, "checkpoints.pth")
    for x, y in Train_data:
        x = x.to(device)
        y = y.to(device)
        y_gen = gen(x)
        disc_fake = disc(x, y_gen.detach())
        disc_real = disc(x, y)
        ones = torch.ones_like(disc_real).to(device)
        zeros = torch.zeros_like(disc_fake).to(device)
        disc_loss = (Log_loss(disc_real, ones) + Log_loss(disc_fake, zeros))
        disc_opt.zero_grad()
        disc_loss.backward()
        disc_opt.step()

        disc_gen = disc(x, y_gen)
        loss = Log_loss(disc_gen, torch.ones_like(disc_gen))
        l1_loss = L1_loss(y_gen, y) * Lambda
        gen_loss = loss + l1_loss
        gen_opt.zero_grad()
        gen_loss.backward()
        gen_opt.step()
    if(i % 5 == 0):
        plt.figure(figsize=(40, 28))  # Increased output size
        with torch.no_grad():
            x, real = next(iter(Train_data))
            fake = gen(x.to(device))
            fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(40, 28))
            for i in range(3):
                ax[0, i].imshow(torchvision.transforms.ToPILImage()(x[i]))
                ax[1, i].imshow(torchvision.transforms.ToPILImage()(real[i]))
                ax[2, i].imshow(torchvision.transforms.ToPILImage()(fake[i]))
                ax[0, i].set_title("input")
                ax[1, i].set_title("real")
                ax[2, i].set_title("predicted")
                ax[0, i].axis('off')
                ax[1, i].axis('off')
                ax[2, i].axis('off')
            plt.tight_layout()
            plt.show()

